	This experiment was a great opportunity to explore how changes to neural network hyperparameters and structure can affect the training speed and evaluation accuracy of the model. For this project I decided to explore the effects the learning rate, the number of hidden layers, and training batch size on neural network performance. Additionally, this experiment shows that while neural networks can be implemented with special libraries such as TensorFlow, they can also viably be implemented using simpler libraries such as Numpy which simply holds the weights as matrices.
	The first hyperparameter tested was learning rate. This hyperparameter has been thoroughly studied, and this experiment served to reinforce many attributes of different learning rates. Generally, the model will converge faster as the learning rate increases. This is readily apparent in the Numpy implementation of the neural network where every increase of the learning rate led to faster convergence. However, setting the learning rate too high can have detrimental effects for network performance. This happens when the learning rate updates the weights to aggressively and the network is not able to settle into the optimal minima solution. The results in lower accuracy and instability from one epoch to the next. This effect can be seen in the TensorFlow implementation. The rate of convergence increased as the learning rate increased from zero to 0.5 to 1. However, when the learning rate increased to 1.5 and then 2 the validation accuracy is lower and less stable from one epoch to the next.
	Second, the number of hidden layers was tested. In deep learning, adding layers to a model allows the network to learn more abstract features before classifying the input. These automatic features can greatly increase the accuracy of networks. However, adding additional hidden layers increases training complexity which means each epoch takes longer to complete then shallower versions. Additionally, more epochs are required for the network to converge. This second trend could be seen in both implementation of the BPNNs. 
	Finally, the effect of batch size was tested. This is not typically considered one of the standard hyperparameters for neural networks; however, the effect of changing this value can clearly be seen in both implementations of the network. This experiment shows that increasing the batch size decreases the rate of convergence for the network. This is because the weights are updated more often during the course of a single epoch. Each of these updates work to move the weights toward the minima. Additionally, as batch size increases the accuracy curve becomes smoother. Again, this has to do with how often the weights are updated during an epoch. Accumulating the error for more training samples allow the weight updates to be closer to the true gradient of the loss function. Updates from smaller batches are more like to deviate from the true gradient and lead to a less smooth training curve. In fact, batch size is what leads to stochastic gradient descent (for small batch sizes) or pure gradient descent (for larger batch sizes).
	Additionally, this experiment appears to show that the TensorFlow implementation converges in fewer epochs than the Numpy implementation. The reason for this is not readily apparent, but I would surmise that TensorFlow has built-in optimizations that accelerate the training process. As such, I would conclude that while neural networks can be implemented using less abstract libraries, TensorFlow and other specialized libraries should be utilized when implementing complex neural network architectures.
	Batch and online processing both refer to different modes of training in machine learning. On the one hand, batch processing averages the errors for multiple or all training samples in an epoch before performing back propagation to update the model parameters – such as neural network weights. Conversely, online processing propagates the error and updates the weights after every individual training sample.
	Gradient descent is performed during batch processing. Accumulating the error for the entire training set yields a more accurate representation of the gradient of the loss function. Stochastic gradient descent corresponds to online training or training when the batch size is considerably smaller than the size of the training set. In this case the weight updates are a less accurate representation of the gradient of the loss function – and thus more stochastic.
	The original model of perceptron neurons used a threshold activation function. If the dot product of inputs and weights exceeded a predetermined value, then the perceptron would emit a one; if not then it would emit a zero. A sigmoid neuron uses a sigmoid activation function. Since this function is continuous and differentiable backpropagation can be employed.
	Neural network training is divided into two distinct phases. The first phase, feedforward, is when an input, or multiple inputs in succession, is fed into a network and a subsequent output is produced. An error or loss is then calculated for the single or batch output. The second phase, backpropagation, is the act of taking the calculated error and, beginning with the output layer, the error and corresponding partial derivatives are used to update the weights of the model.

	Nielsen’s code already had much of the functionality that was necessary for this task; however, I did modify his code slightly. Additionally, I added code to implement the softmax activation for the output as well as the log-likelihood cost function. The following output activations and cost functions were tested. For each combination the network was trained three times, and the average costs and epochs from those three trials were plotted. The following section contains the plots for these averages. This task allows us to observe what is known as the “learning slowdown problem”. 
	The learning slowdown problem is an issue that can plague neural network practitioners. The problem stems from scenarios where the partial derivative of the cost function with respect to the weights or biases contains the derivative of the activation function. Activation functions, such as the sigmoid function, have regions where it “saturates”. The slope in these saturation regions of the activation function is very small. Since weight and bias updates depend on this derivative, the update can be very small for neurons with saturate activations. These small changes in the training stage mean that the network needs more epochs in order to converge – the network learns more slowly.
	When activation functions are coupled with appropriate cost functions the derivative of the activation function will be cancelled out by terms in the partial derivative of the cost function. Removing the activation derivative means that the network will no longer be slowed down by saturated activations in the network.
	Sigmoid activation and cross entropy cost functions are an example of this more ideal pairing. Softmax activation and log-likelihood cost functions are another. Indeed, the graphs above serve to show how the networks with these pairs of functions train much faster than the network with sigmoid activation and quadratic cost function. The sigmoid, cross entropy pair take between 40 and 50 epochs before they reach convergence. However, the other two pairs only require 10 to 20 before they converge. The softmax, log-likelihood network converges so fast that it even begins to overfit. While the sigmoid, cross entropy and softmax, log-likelihood pairs had lower average test accuracy for these trials, they would most certainly excel if smaller learning rates were chosen.
	For this task several different coefficients for L1 and L2 regularization terms were tested to see how they affect test performance of the network. The effects of regularization are susceptible to batch size. As such four different magnitudes were tested to ensure that one of them would be adequate for the chose batch size of 100 for these trials.
	Additionally, the network was trained with an augmented dataset for the third trial. This dataset was generated using a revised version of Nielsen’s code. Each training image was duplicated four times. Each copy was then moved randomly by up to three pixels in both the x and y directions. Each copy was then rotated by up to 45 degrees. 
	Overfitting is when a model begins to lose generalizability as it attempts to fit the training data as closely as possible. When this occurs, the model becomes less reliable for accurately classifying new samples. Colloquially, this is often equated to memorization versus learning. The network memorizes the training examples instead of learning the trends that will allow it to handle new cases.
	L1 and L2 regularization are two methods for ensuring that a network does not overfit, or memorize, the training data. They are actually terms that get added to the cost function. They serve to increase the total cost or loss as network weights become larger. The rationale is that networks with larger weights may be working to fit training points that are outliers from the general trend. The added term to the loss function ensures that large weights will appear only if it serves to significantly reduce the original term in the loss function. L1 and L2 serve the same purpose but the penalize weights in different ways. L1 increases the loss proportional to the sum of absolute weights; whereas L2 increases the loss proportional to the sum of squared weights.
	Regularization is a constraint-based way to improve testing classification accuracy – it constrains the values of weights to keep them small. Another method is to increase or augment the dataset with new samples. Ideally, this augmented training set will give the network a better chance to learn the general trends of a problem. With enough samples a network will not be able to move towards rote memorization and will be forced to learn relationships between features. In this experiment, the augmented dataset did not help the network learn any better. This may stem from how drastic the perturbations were when creating new training samples. This may have an effect throughout the following tasks.
	This task explores how adding hidden layers affects a network’s training and testing performance. First one hidden layer was added, then two hidden layers. Each hidden layer contained 30 neurons. Additionally, the rate of learning for each of the two hidden layers was tracked throughout learning to explore the unstable gradient problem. Finally, dropout was implemented and test to see how it affects test accuracy.
	Adding layers is a great way to introduce complexity into a neural network. It allows the network to engineer abstract features at each layer which is useful for complicated tasks such as hand writing recognition. Figures 9 and 10 show the boost in accuracy that can be achieved by allowing the network to learn more complex relationships within the training data. However, each additional layer makes the network more difficult and slower to train. This is due to the unstable gradient problem. This problem arises from the fact that the gradient is unstable as it propagates backwards through the hidden layers. Layers near the output layer tend to train quickly – at times leading to an exploding gradient. On the other hand, hidden layers closer to the input layer have smaller gradients to update from due to the vanishing gradient problem. Figure 11 shows this nicely. Throughout training the updates to nodes in first hidden layer update more slowly than nodes in the second hidden layer. 
	In the previous task L1 and L2 regularization were discussed. Dropout is another way to ensure that a network does not overfit. It is executed by turning off nodes randomly in the hidden layers during training of the network from one batch to the next. Turning on and off connections in this fashion while training ensures that the network cannot memorize the training data because no weight is guaranteed to be active for any given batch. Additionally, this technique is similar to training several networks on the data independently and then using them all as an ensemble when testing. This is because many different redundant pathways have been trained to perform the classification within the single network. However, figure 14 shows that a dropout rate that is too high will keep the network from learning any meaningful relationships while training.
	For this task the LeNet architecture was used to perform the same MNIST task as before. The goal was to find a set of hyperparameters that yields the highest classification accuracy. The hyperparameters are as follows. A modified parameter sweep method was used to accomplished this. The sweep began by iterating over and test the initial test values for ‘batch size’ while ‘keep probability’ and ‘learning rate’ were held constant at their respective initial best value. The best ‘batch size’ from the first sweep was stored. The second sweep was over ‘keep probability’ with ‘batch size’ and ‘learning rate’ held constant, and the new best ‘keep probability’ was stored. The third sweep worked the same for ‘learning rate’. This process was repeated two more times over smaller neighborhoods around the current best value for each hyperparameter. Additionally, the test was run twice in order to compare the results of the method. The following section shows results the parameter sweep for the two trials.
	LeNet shows a significant improvement in classification accuracy over the BPNN. LeNet’s many layers allow it to learn more complex and abstract features about the structure of the different numbers. The BPNN with extra hidden layers benefits from this feature abstraction but not as much and not in the same way. LeNet’s learning is more robust because it focuses its learning on local interactions between neighboring pixels. These relationships are then combined to form the more complex features. BPNN on the other hand attempts to learn the full enumeration of pairwise combinations of pixels. As a result, it would require any more layers – and a lot of regularization – to begin approaching the efficacy of LeNet.

	The objective of this project was to implement, train, and deploy VGGNet. This network was originally developed for the ImageNet classification task; however, this project focused on utilizing it for the CIFAR-10 classification task. The weights from the original instance of VGGNet are widely available, so this project also looked into transfer learning ¬– using learned weights from one domain and applying them to a problem of a different domain. This paper will cover the various attempts to train VGGNet for CIFAR-10. The first half contains the attempts to complete the two tasks in the problem statement; the second half contains attempts to tweak the network and dataset in order to achieve a higher accuracy. 
	convolutional layers, max pooling layers, and fully connected layers. Additionally, it uses ReLU activations for every layer except the output, which uses SoftMax. The model was originally created for ImageNet takes input images of 224x224 pixels, and CIFAR-10 images are 32x32 pixels. As a result, images had to be resized appropriately before being passed into the network. The dataset consisted of 40,000 training images, 10,000 validation images, and 10,000 testing images. Additionally, I implemented a form of learning rate decay for these experiments. If the model did not improve accuracy for three epochs, then the learning rate would be decreased by a constant factor. When the model did not improve for six epochs early stopping would end training and initiate testing. This is why each model ran for a different number of epochs. The training details for the first two models are listed below, and table 1 enumerates the difference between the two models. Batch size of 50, Initial learning rate of 1e-5, Learning rate decay of 0.75, Stochastic gradient descent optimizer. Finally, the training accuracy reporting only represent the accuracy for a batch of fifty images. 
	For this model VGGNet’s weights from ImageNet were loaded into the model. Then, only weights from the last three groups of layers were allowed to train. These layers included all of the fully connected layers, all of the layers in the fifth convolution group, and all of the layers in the third convolution group. This was done not only so the model would be able to leverage the low-level filters that were learned by VGGNet for ImageNet, but also to allow VGGNet to adapt the more abstract learned features to the specific needs of CIFAR-10. Figure 1 shows the accuracy results of training VGGNet from the original weights. The performance of this trial was better than training the same network starting from random weights. 
	The initial model from scratch was set up the same as the previous model. The only difference is that all of the weights of the model were randomly initialized. Figure 2 shows the accuracy results of training VGGNet randomly initialized weights.
	For the second phase I tried augmenting the network in the phase 1 code, and I also added a form of dataset augmentation. This portion consisted of creating networks of two different depths – 16 layers like the original and a smaller 9-layer network. These networks also takes the CIFAR-10 images as their native 32x32 pixel images. I also added batch normalization and dropout to the network. Batch normalization can speed up the rate of convergence of a network and dropout helps to discourage overfitting. Both of these methods have been shown to improve the performance of CNNs. The rest of the experiment settings are shown below; table 2 outlines the differences between the two models.
	The 16-layer network had all of the convolutional, pooling, and fully connected layers of the original; the 9-layer network removed the third and fourth convolutional modules and one of the fully connected layers. Batch normalization was included after every convolutional layer and fully connected layers except the output layer. Additionally, the dropout was used after the first two fully connect layers.
	Additionally, every test image was shifted horizontally and vertically anywhere from -3 to 3 pixels in each direction. Finally, each training image had a small amount of Gaussian noise added to it. Both of these additions served to augment the base 40,000 test images Note that each epoch still only consisted of a pass through 40,000 images with these perturbations.
	This project gave great insight on how to train and utilize a truly deep network. It demonstrated the importance of using techniques such as batch normalization and dropout in order to wrangle the difficulties associated with training a deep neural network. These techniques led a 33.5% performance between the best phase 1 network and best phase 2 network. The smaller network performed best, but this is probably due, in part, to user error. Consistently deeper networks have been shown to yield higher performance and accuracies.
	LeNet was the first network that applied convolutional layers to the task of image recognition. The design consisted of sets of convolutional layers and pooling layers followed by fully connected layers. This became the ubiquitous design for CNNs for image recognition. This design was influential because it is more scalable than using fully connected networks for image recognition tasks. They accomplish by focusing on interactions and relationships between inputs that are spatially close together within a receptive field. They used a field or kernel with size 5x5. Additionally, kernels that were used in computer vision were hand generated, but LeNet shows how these kernels can be learned directly from the images.
	AlexNet was the first CNN that really had a wide-spread impact on the field of computer vision. While other neural networks were being run on GPUs, they were able to create an architecture that effectively leveraged two GPUs simultaneously. The network was introduced at the annual ImageNet competition and outperformed traditional computer vision models for classification and other tasks. Additionally, it extended the paradigm that was set forth with LeNet with its series of convolutional and pooling layers followed by fully connected layers at the end. With this network came the introduction of rectified linear unit, or ReLU as an activation. This new activation served to drastically speed up training time while contributing the required non-linearity for CNNs to learn complex problems. They also use a larger receptive field than LeNet – 11x11 pixels. They also integrated regularization through the use of dataset augmentation and dropout.
	VGGNet took the traditional CNN architecture to the extreme by having 16 trainable layers. They went deeper but “thinner” in some respects. Where the last two models used kernel sizes of 5x5 and 11x11, VGGNet’s kernel was only 3x3 pixels. This architecture truly demonstrated the power and flexibility of the classic CNN structure. This model has been utilized for many other image recognition tasks. In short, while VGGNet was much deeper than other networks, it was still simple, and it had great performance.
	Unlike VGGNet, GoogLeNet deviated significantly from the traditional CNN model. The network was an astounding 22 layers, and they introduced a construct called an inception model. This new module was an example of the concept of “network-in-network”. Each inception module performs several operations in parallel. They consist of 1x1, 3x3, and 5x5 convolution layers in parallel as well as a 3x3 max pooling in parallel. This allows the network to learn features at several different resolutions. Additionally, the authors claim that this architecture achieved better accuracy with lower additional computational and memory complexity increases than past networks had achieved. 
	ResNet also took neural networks to new depths with a 152-layer architecture. As other networks grew deeper, they often became hindered with problems such as vanishing or exploding gradient and other degradation issues. The authors of this paper were able to alleviate these issues by creating a network that would not have to learn direct mappings of inputs to outputs at every layer; instead each module of the network would only have to learn a residual mapping H(x) that, when added to the original input (x) to that module, would equal the desired mapping F(x). This was easily accomplished by simply feeding the input of one module through the module and around the module and add it back to the output to be fed into the next layer. This simple structure did not increase the complexity (number of parameters) of the model at all, and it also mitigated the problems that plagued typical deep architectures. Subsequently, this allowed for much deeper networks that could learn and leverage much more complex features from images and achieve higher accuracies.
	This newest work aims to intermingle the local and global features learned by a network. The SE modules can be dropped in after any convolutional layer; The authors were even able to extend GoogLeNet and ResNet with these modules. They serve to take the features learned in each output channel, create an embedding, and then use that embedding, in turn, to weight or emphasize certain channels. These SE modules are another example of a “network-in-network” structure. The embeddings are simple averages across all “pixels” for each channel (HxWxC to 1x1xC). These embeddings are sent through a fully connected layer, ReLU activation, fully connected layer, and then sigmoid activation. The final result is a 1x1xC set of weights that are used to scale the original HxWxC outputs that will then be fed into the next layer. Ultimately, the ability to capture channel-wise feature dependencies improves the performance for deep architectures.

	This project focused on denoising autoencoders. Autoencoders are interesting because they are trained in an unsupervised fashion – no class labels are required. Instead, each image acts as its own target during network training. Using this technique reveals interesting latent representations for complex datasets. The project also explores using these autoencoders as preprocessing units for other neural networks and as a technique to pretrain hidden layers within a deep neural network.
	The first task consisted of training a denoising autoencoder on the CIFAR-10 dataset. Each image was first standardized by dividing each pixel in each channel by 255. The data was then corrupted with Gaussian noise with a standard deviation of 0.05. The DAE was six layers deep; three encoding layers and three decoding layers. The layer dimensions were 2048, 1024, and 512 and mirrored for the decoder. ReLU activation was used for every layer, and the network was trained for 150 epochs. Figure 1 shows the training loss, and figures 2-5 show visual progression of the DAE training. They show the raw images, the noisy images, and the output from the DAE.
	The denoised images are significantly more noisy than the original images. Much of the shading of the original image is apparent in the decoded images but not much else. All of the other features are significantly blurred, and all of the images exhibit pixels with high intensities spread throughout the image.
	For this portion of the experiment we attempted to utilize the DAE as a preprocessing step for images. The idea is that the DAE would be able to remove noisy features from the image and let the most salient components of the image stand out. Figure 6 shows some sample images from the test set. The bottom row is the output from the DAE that was then used as input into the tiny-VGGNet. The network is composed of a convolutional and max pooling layers, another convolutional and max pooling layers, two additional convolutional and a max pooling layer and then a fully connected layer for the classification. Figure 7 shows the training curve for tiny-VGGNet. Additionally, it shows the performance on the raw test images and the test images that were passed through the DAE. 
	The accuracy was significantly worse for the denoised images. Figure 6 shows that many of the sharp features are lost during the denoising process. Additionally, the saturated pixels of the processed images also served to stump tiny-VGGNet. For this technique to be effective the network would have to be trained using the denoised images or the DAE would have to be able to create much more crisp images. 
	For this task we had to train the tiny-VGGNet one layer at a time in an unsupervised fashion. Starting with the first convolutional layer, this entailed treating each individual layer as if it were a two-layer autoencoder – one encoding layer and an additional decoding layer. Once one layer is trained, the following layer may be trained by propagating the inputs through previously trained layers and then using the input to the new layer as the targets for its training.
	The results of this process were similar to the baseline end-to-end training of tiny-VGGNet as seen in figure 8. The accuracy begins so high because of the successful pretraining of the weight for tiny-VGGNet; however, the results never match nor exceed the final results from the baseline training.
	For this project we explored the theory behind denoising autoencoders and their use as a pretraining method for deep networks – even if we did not confirm those findings in our implementations. They can be leveraged in a powerful way to ensure that networks do not get stuck in poor local minima. Using noisy inputs require the network to learn generalized features that are most important for representing the information within the images. This also lends to their efficacy as a pretraining technique.
	Supervised and unsupervised learning are two concepts that underpin machine learning. They are both powerful classes of methods that can yield useful information for many applications; however, they are diametrically opposite in how they are implemented and how they learn patterns from datasets. Supervised learning depends on samples of data belonging to distinct classes. The algorithms then learn relationships within and between classes. Unsupervised learning on the other hand seeks to find interesting relationships within the data itself and not in relation to any imposed class.
	People have found ways to combine supervised and unsupervised methods for effective training for neural networks. Traditionally, NN’s are trained in a supervised fashion. Back propagation that relies on calculating a gradient of an error function with respect to the difference between the network output and the corresponding training label. However, autoencoders have been shown to be an effective unsupervised pretraining method for NN’s. It is unsupervised because it does not require an additional class label for training. Instead, the error is calculated with respect to the output from the network and the original input. Denoising autoencoders have been shown to effectively learn salient features from images that can then be used for classification. Its efficacy is in part due to the fact that training a classifier after DAE pretraining helps ensure that the network does not get caught in a poor local minima. However, new techniques such as dropout, batch normalization, and other activation functions have mitigated much of the arguments in support of pretraining. 
	Variational autoencoders are yet another interpretation on the autoencoding framework. In short they force the latent representation of an image to be from a learned distribution (mean and standard deviation are learned). This learned distribution can then be used to generate additional images that are not from the original dataset.

	GANs are an intriguing advancement in neural networks. They pit two networks against each other – one trying to generate samples that mimic a dataset and another to detect these fakes. The overall system allows for two networks that each have interesting characteristics over and above traditional neural networks. One will learn to generate eerily realistic samples and the other will have to potential to classify real samples into the correct classes as well as detect false and even malicious examples. This experiment explores two of the most common forms of these networks: the DCGAN and the conditional DCGAN.
	Using code from [1], we were able to train a DCGAN on the MNIST dataset. One note is that this code has routines for MNIST built in that trains a conditional GAN. So, in order to train an unconditional GAN we had to download MNIST outside of the built-in routine and then treat it as a custom dataset. The results of the training are shown in figures 1 trough 3 below.
	For this portion of the experiment we were able to use the routines built in to the code from [1]. MNIST is one of the built-in datasets for training of a conditional GAN. The results from this training can be seen in figures 4 through 6 below.
	Much of the complications with training GANs is the fact that two networks must be trained simultaneously. this can lead to several different issues where the generator performs worse than expected. The training can be plagued by mode dropping, vanishing gradients, or unstable updates for the generator. This instability appears to stem from the fact that the two networks learn while engaged in a minimax game. Intuitively, then, one can imagine that as the discriminator learns the generated samples of the generator, the generator will begin to map noise to different modes in an attempt to fool the discriminator. This can be seen in figure 1 where a single noise vector produces different numbers depending on the epoch. The generator and discriminator end up cycling around the modes – oscillating and never converging. Another problem arises when the generator maps most of the noise to a single mode that happens to be effective at fooling the discriminator. Instead of accurately representing the real distribution, the generator drops certain modes or classes that it is unable to produce convincing samples of. Additionally [Arjovsky:2017]	, discusses the phenomenon where the gradient for the generator vanishes as the discriminator’s loss improves.
	[DCGAN:2016] introduces some constraints for convolutional GANs that they claim reduce the instability when training the GAN’s. The proposed constraints are replacing pooling layers with strided convolutions, using batch normalization, removing fully connected hidden layers, utilizing ReLU for all layers except output of the generator, and using leaky ReLU for the discriminator. These techniques had been used successfully in state-of-the-art convolutional architectures in general.
	In [] conditional GANs are introduced. They aim to mitigate the instability and mode dropping problems by adding additional information to the noise vector and discriminator input. They can then condition the training of the networks on this additional information. The authors note that any information can be used to condition the data, but commonly a class label is used. Then, generators have to create an output given that it is supposed to belong to a specific class. Likewise, the discriminator has to determine the veracity of its input given that it belongs to class. Figure 4 shows the benefits of this scheme. The images are much clearer and more stable – every sample that uses the same noise vector produces the same number across samples.
